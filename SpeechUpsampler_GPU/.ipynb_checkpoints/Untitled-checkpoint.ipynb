{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import scipy.io.wavfile as wav\n",
    "from tensorflow.python.lib.io import file_io\n",
    "import numpy as np\n",
    "\n",
    "def decode_csv(line):\n",
    "\n",
    "    parsed_line = tf.decode_csv(line, [['string'], ['string']])\n",
    "    feat = parsed_line[0]\n",
    "    lab = parsed_line[-1]\n",
    "\n",
    "    return feat, lab\n",
    "\n",
    "def audiofile_to_input_vector(truth_file):\n",
    "\n",
    "#    Load wav files (1st column)\n",
    "#    binary_truth = file_io.FileIO(truth_file, 'b')\n",
    "    fs_truth, audio_truth = wav.read(truth_file)    \n",
    "    return audio_truth.astype( np.float32 ).reshape(-1,1) \n",
    "\n",
    "\n",
    "def input_parser(truth_file, ds_file):\n",
    " \n",
    "    truth_audio_array = tf.py_func(audiofile_to_input_vector,[truth_file], [tf.float32])\n",
    "    ds_audio_array = tf.py_func(audiofile_to_input_vector,[ds_file], [tf.float32])     \n",
    "    \n",
    "    return truth_audio_array[0] , ds_audio_array[0] \n",
    "\n",
    "\n",
    "# Training pipeline\n",
    "\n",
    "def train_input(data_filepath, batch_size, num_epoch):\n",
    "   \n",
    "\n",
    "#   num_parallel_calls = cpu_count()\n",
    "    dataset = tf.data.TextLineDataset(data_filepath).map(decode_csv, num_parallel_calls = 1)\n",
    "    dataset = dataset.map(input_parser,num_parallel_calls=4)\n",
    "#    dataset = dataset.apply(tf.contrib.data.ignore_errors())\n",
    "    dataset = dataset.repeat(num_epoch)\n",
    "    dataset = dataset.padded_batch( batch_size, padded_shapes=( [None,1], [None,1] ) )\n",
    "#     dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.prefetch(batch_size)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    upsampled, downsampled = iterator.get_next()\n",
    "    return upsampled, downsampled\n",
    "\n",
    "\n",
    "# Inference pipeline\n",
    "def decode_csv_inference(line):\n",
    "\n",
    "    parsed_line = tf.decode_csv(line, [['string']])\n",
    "    wave_name = parsed_line[0]\n",
    "\n",
    "    return wave_name\n",
    "\n",
    "\n",
    "\n",
    "def audiofile_to_input_vector_inference(truth_file):\n",
    "\n",
    "#    Load wav files (1st column)\n",
    "#    binary_truth = file_io.FileIO(truth_file, 'b')\n",
    "    fs_truth, audio_truth = wav.read(truth_file)    \n",
    "    return audio_truth.astype( np.float32 ).reshape(-1,1) \n",
    "\n",
    "\n",
    "def input_parser_inference(truth_file):\n",
    " \n",
    "    truth_audio_array = tf.py_func(audiofile_to_input_vector_inference,[truth_file], [tf.float32])    \n",
    "    \n",
    "    return truth_audio_array[0]\n",
    "\n",
    "\n",
    "# Training pipeline\n",
    "\n",
    "def inference_input(data_filepath):\n",
    "   \n",
    "\n",
    "#   num_parallel_calls = cpu_count()\n",
    "    dataset = tf.data.TextLineDataset(data_filepath).map(decode_csv_inference, num_parallel_calls = 1)\n",
    "    dataset = dataset.map(input_parser_inference,num_parallel_calls=4)\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    waveform = iterator.get_next()\n",
    "    return waveform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filepath = '/home/mohit/Desktop/Research papers/Implementation/SpeechUpsampler_GPU/data/upsampling/upsampling_data/upsampling.csv'\n",
    "\n",
    "waveform = inference_input(filepath)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    for i in range(10):\n",
    "        try:\n",
    "            array = sess.run(waveform)\n",
    "            print(array.shape)\n",
    "        except tf.errors.OutOfRangeError as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import losses\n",
    "from optimizers import make_variable_learning_rate, setup_optimizer\n",
    "import models\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pipeline\n",
    "import time\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "upsampling_settings_file = 'settings/upsampling_settings.json'\n",
    "upsampling_settings = json.load(open(upsampling_settings_file))\n",
    "model_checkpoint_file_name = os.getcwd() + upsampling_settings['model_checkpoint_file']\n",
    "upsampling_factor = 2\n",
    "\n",
    "def read_csv(filepath):\n",
    "    df = pd.read_csv(filepath, header = None)\n",
    "    return len(df)\n",
    "\n",
    "\n",
    "upsample_filepath = './data/upsampling/upsampling_data/upsampling.csv'\n",
    "UPSAMPLING_LENGTH = read_csv(upsample_filepath)\n",
    "\n",
    "\n",
    "\n",
    "df = pd.read_csv(upsample_filepath, header = None)\n",
    "\n",
    "true_wf, true_br = librosa.load(df.iloc[0,0], sr=None, mono=True)\n",
    "\n",
    "us_wf = librosa.core.resample(true_wf, true_br, true_br*upsampling_factor)\n",
    "us_br = true_br*upsampling_factor\n",
    "\n",
    "print(true_br,us_br, true_wf.dtype)\n",
    "\n",
    "\n",
    "\n",
    "# Define placeholder for input file\n",
    "waveform = pipeline.inference_input( upsample_filepath)\n",
    "inp_waveform = tf.placeholder(tf.float32, shape = [1,16000,1])\n",
    "_,_, y = models.inference(inp_waveform, inp_waveform.get_shape().as_list())\n",
    "\n",
    "# saver = tf.train.Saver()\n",
    "\n",
    "\n",
    "# with tf.Session() as sess:\n",
    "\n",
    "#     saver.restore(sess, model_checkpoint_file_name)\n",
    "#     waveform = pipeline.inference_input( validation_filepath)\n",
    "\n",
    "#     waveform_eval =  sess.run(waveform)     \n",
    "        \n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-1-34119b01521f>, line 80)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-34119b01521f>\"\u001b[0;36m, line \u001b[0;32m80\u001b[0m\n\u001b[0;31m    print('Model created\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import losses\n",
    "from optimizers import make_variable_learning_rate, setup_optimizer\n",
    "import models_exp\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pipeline\n",
    "import time\n",
    "\n",
    "\n",
    "import scipy.io.wavfile as wav\n",
    "\n",
    "\n",
    "upsampling_factor = 2\n",
    "\n",
    "upsampling_path = './data/upsampling/upsampling_data/'\n",
    "\n",
    "\n",
    "#custom_shuffle_module = tf.load_op_library('src/shuffle_op.so')\n",
    "#shuffle = custom_shuffle_module.shuffle\n",
    "\n",
    "try:\n",
    "    os.makedirs('./aux/checkpoint')\n",
    "except Exception: \n",
    "    pass\n",
    "\n",
    "data_settings_file = 'settings/data_settings.json'\n",
    "training_settings_file = 'settings/training_settings.json'\n",
    "model_settings_file = 'settings/model_settings.json'\n",
    "\n",
    "data_settings = json.load(open(data_settings_file))\n",
    "training_settings = json.load(open(training_settings_file))\n",
    "model_settings = json.load(open(model_settings_file))\n",
    "\n",
    "# Constants describing the training process.\n",
    "# Samples per batch.\n",
    "BATCH_SIZE = training_settings['batch_size']\n",
    "# Number of epochs to train.\n",
    "NUMBER_OF_EPOCHS = training_settings['number_of_epochs']\n",
    "# Epochs after which learning rate decays.\n",
    "NUM_EPOCHS_PER_DECAY = training_settings['num_epochs_per_decay']\n",
    "# Learning rate decay factor.\n",
    "LEARNING_RATE_DECAY_FACTOR = training_settings['learning_rate_decay_factor']\n",
    "# Initial learning rate.\n",
    "INITIAL_LEARNING_RATE = training_settings['initial_learning_rate']\n",
    "\n",
    "example_number = 0\n",
    "write_tb = False\n",
    "\n",
    "file_name_lists_dir = data_settings['output_dir_name_base']\n",
    "train_filepath =  './data/preprocessed/train_files.csv'\n",
    "validation_filepath = './data/preprocessed/validation_files.csv'\n",
    "\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    with tf.Session() as sess:\n",
    "        x = tf.placeholder(dtype = tf.float32,shape = [None,16000,1])\n",
    "        train_flag, x , up_audio_return_y = models_exp.train( input_data = x, \n",
    "                                                        input_shape = x.get_shape())\n",
    "        \n",
    "        \n",
    "        def read_file(wave, path):\n",
    "            br, array = wav.read(path + wave)\n",
    "            array = librosa.core.resample(array.astype(np.float32), br, br*upsampling_factor)\n",
    "            array = np.pad(array,(0,16000-len(array)%16000), 'constant').reshape(-1,16000,1)\n",
    "            array_tf = tf.convert_to_tensor(array)\n",
    "            return array_tf\n",
    "\n",
    "\n",
    "        for file in os.listdir(upsampling_path):\n",
    "            if file.endswith('wav'):\n",
    "                array_tf = read_file(file, upsampling_path)\n",
    "        \n",
    "        print('Model created')\n",
    "        \n",
    "        \n",
    "        saver = tf.train.Saver()\n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import models_inference as mi\n",
    "\n",
    "\n",
    "data_settings_file = 'settings/data_settings.json'\n",
    "model_settings_file = 'settings/model_settings.json'\n",
    "upsampling_settings_file = 'settings/upsampling_settings.json'\n",
    "\n",
    "data_settings = json.load(open(data_settings_file))\n",
    "model_settings = json.load(open(model_settings_file))\n",
    "upsampling_settings = json.load(open(upsampling_settings_file))\n",
    "\n",
    "upsample_csv = upsampling_settings['input_file']+ upsampling_settings['filename']\n",
    "\n",
    "source_dir = os.path.split(upsample_csv)[0]\n",
    "\n",
    "\n",
    "END_OFFSET = data_settings['end_time']\n",
    "upsampling_factor = 2\n",
    "INPUT_SIZE = int(data_settings['downsample_rate'])*upsampling_factor\n",
    "\n",
    "\n",
    "\n",
    "model_checkpoint_file_name = os.getcwd() + upsampling_settings['model_checkpoint_file']\n",
    "\n",
    "\n",
    "# Iterate through complete upsampling list and convert\n",
    "df = pd.read_csv(upsample_csv, header = None)\n",
    "\n",
    "true_wf, true_br = librosa.load(df.iloc[0,0], sr=None, mono=True)\n",
    "\n",
    "us_wf = librosa.core.resample(true_wf, true_br, true_br*upsampling_factor)\n",
    "us_br = true_br*upsampling_factor\n",
    "\n",
    "train_flag, x, model = mi.deep_residual_network(true_wf.dtype,\n",
    "                                                 np.reshape(us_wf, (-1,1))[:us_br].shape,\n",
    "                                                 **model_settings)\n",
    "\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "corrupt_file = []\n",
    "with tf.Session() as sess:\n",
    "        # create session and restore model\n",
    "    saver.restore(sess, model_checkpoint_file_name)\n",
    "\n",
    "    for index in range(len(df)):\n",
    "        # Load 8k file\n",
    "        true_wf, true_br = librosa.load(df.iloc[index,0], sr=None, mono=True)\n",
    "\n",
    "        # Upsampled file. This will be fed into Deep Neural Network\n",
    "        us_wf = librosa.core.resample(true_wf, true_br, true_br*upsampling_factor)\n",
    "        us_br = true_br*upsampling_factor\n",
    "        number_of_reco_iterations = int(us_wf.size/INPUT_SIZE)\n",
    "\n",
    "\n",
    "\n",
    "        # ###################\n",
    "        # RECONSTRUCTION LOOP\n",
    "        # ###################\n",
    "\n",
    "        reco_wf = np.empty(us_wf.size)\n",
    "        print('Processing:',df.iloc[index,0])\n",
    "        # Calculate number of samples in test file\n",
    "        for i in range(number_of_reco_iterations):\n",
    "            print('Segement {} of {}'.format(i + 1, number_of_reco_iterations))\n",
    "        # Feeding \n",
    "            example = us_wf[i*INPUT_SIZE:(i + 1)*INPUT_SIZE]\n",
    "            reco_wf[i*INPUT_SIZE:(i + 1)*INPUT_SIZE] = model.eval(feed_dict={train_flag: False,\n",
    "                                                                             x: example.reshape(1, -1, 1)},\n",
    "                                                                  session=sess).flatten()\n",
    "\n",
    "        file_name_base = df.iloc[index,0].split('/')[-1]\n",
    "        \n",
    "        try:\n",
    "            librosa.output.write_wav(os.path.join(source_dir, 'reco_' + file_name_base),y=reco_wf, sr=us_br)\n",
    "        except Exception:\n",
    "            corrupt_file.append(df.iloc[index,0])\n",
    "            pass\n",
    "        \n",
    "df = pd.DataFrame(corrupt_file)\n",
    "corrupt_file.to_csv('currupt_files.csv', header = None, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.io.wavfile as wav\n",
    "import path\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upsampling_factor = 2\n",
    "\n",
    "upsampling_path = './data/upsampling/upsampling_data/'\n",
    "def read_file(wave, path):\n",
    "    br, array = wav.read(path + wave)\n",
    "    array = librosa.core.resample(array.astype(np.float32), br, br*upsampling_factor)\n",
    "    array = np.pad(array,(0,16000-len(array)%16000), 'constant').reshape(-1,16000,1)\n",
    "    array_tf = tf.convert_to_tensor(array)\n",
    "    return array_tf\n",
    "    \n",
    "    \n",
    "for file in os.listdir(upsampling_path):\n",
    "    if file.endswith('wav'):\n",
    "        print(read_file(file, upsampling_path))\n",
    "        \n",
    "        \n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "import losses\n",
    "from optimizers import make_variable_learning_rate, setup_optimizer\n",
    "import models\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import pipeline\n",
    "import time\n",
    "\n",
    "#custom_shuffle_module = tf.load_op_library('src/shuffle_op.so')\n",
    "#shuffle = custom_shuffle_module.shuffle\n",
    "\n",
    "try:\n",
    "    os.makedirs('./aux/checkpoint')\n",
    "except Exception: \n",
    "    pass\n",
    "\n",
    "data_settings_file = 'settings/data_settings.json'\n",
    "training_settings_file = 'settings/training_settings.json'\n",
    "model_settings_file = 'settings/model_settings.json'\n",
    "\n",
    "data_settings = json.load(open(data_settings_file))\n",
    "training_settings = json.load(open(training_settings_file))\n",
    "model_settings = json.load(open(model_settings_file))\n",
    "\n",
    "# Constants describing the training process.\n",
    "# Samples per batch.\n",
    "BATCH_SIZE = training_settings['batch_size']\n",
    "# Number of epochs to train.\n",
    "NUMBER_OF_EPOCHS = training_settings['number_of_epochs']\n",
    "# Epochs after which learning rate decays.\n",
    "NUM_EPOCHS_PER_DECAY = training_settings['num_epochs_per_decay']\n",
    "# Learning rate decay factor.\n",
    "LEARNING_RATE_DECAY_FACTOR = training_settings['learning_rate_decay_factor']\n",
    "# Initial learning rate.\n",
    "INITIAL_LEARNING_RATE = training_settings['initial_learning_rate']\n",
    "\n",
    "example_number = 0\n",
    "write_tb = False\n",
    "\n",
    "file_name_lists_dir = data_settings['output_dir_name_base']\n",
    "train_filepath =  './data/preprocessed/train_files.csv'\n",
    "validation_filepath = './data/preprocessed/validation_files.csv'\n",
    "\n",
    "def read_csv(filepath):\n",
    "    df = pd.read_csv(filepath)\n",
    "    return len(df)\n",
    "\n",
    "\n",
    "\n",
    "SAMPLES_PER_EPOCH_TRAIN = read_csv(train_filepath)\n",
    "SAMPLES_PER_EPOCH_VALID = read_csv(validation_filepath)\n",
    "\n",
    "\n",
    " #### my code #### \n",
    "current_data = []\n",
    "\n",
    "\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    global_step = tf.train.get_or_create_global_step()\n",
    "    with tf.Session() as sess:\n",
    "        \n",
    "#         Training pipeline\n",
    "        inp = pipeline.train_input( train_filepath, BATCH_SIZE, NUMBER_OF_EPOCHS )\n",
    "\n",
    "        saver = tf.train.Saver()\n",
    "        up_audio , down_audio = inp\n",
    "\n",
    "        up_audio_eval =  sess.run(up_audio)\n",
    "        train_flag, x , up_audio_return_y = models.inference( input_data = up_audio , \n",
    "                                                                input_shape = up_audio_eval.shape)\n",
    "\n",
    "#         Calculate loss.\n",
    "\n",
    "        loss = losses.mse(\"loss\",up_audio, up_audio_return_y)\n",
    "\n",
    "#         Variable that affect learning rate.\n",
    "        num_batches_per_epoch_train = int(SAMPLES_PER_EPOCH_TRAIN/BATCH_SIZE)\n",
    "        print('num_batches_per_epoch:',num_batches_per_epoch_train)\n",
    "        decay_steps = int(num_batches_per_epoch_train * NUM_EPOCHS_PER_DECAY)\n",
    "\n",
    "\n",
    "        # Decay the learning rate based on the number of steps.\n",
    "        lr, global_step = make_variable_learning_rate(INITIAL_LEARNING_RATE,\n",
    "                                                      decay_steps,\n",
    "                                                      LEARNING_RATE_DECAY_FACTOR,\n",
    "                                                      False)\n",
    "\n",
    "        min_args = {'global_step': global_step}\n",
    "\n",
    "#             Defining optimizer\n",
    "        train_step = setup_optimizer(lr, loss, tf.train.AdamOptimizer,\n",
    "                                     using_batch_norm=True,\n",
    "                                     min_args=min_args)\n",
    "        \n",
    "        \n",
    "#             Training Loop\n",
    "        try:\n",
    "        \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            training_loss = open('train_loss.txt', 'w')\n",
    "            valid_loss = open('validation_loss.txt', 'w')\n",
    "            for i in range(NUMBER_OF_EPOCHS):\n",
    "                for j in range(num_batches_per_epoch_train):\n",
    "                    avg_loss = 0\n",
    "                    try:\n",
    "                        _, loss_value = sess.run([train_step, loss, ])\n",
    "                        avg_loss +=  loss_value\n",
    "                        if j % 50 == 0:\n",
    "                            print( 'Training Loss in epoch {} and batch {} is {}:'.format(i+1,j+1, \n",
    "                                                                                          avg_loss/(j+1) ))\n",
    "                    except tf.errors.OutOfRangeError :\n",
    "                        print('Data used')\n",
    "                        pass\n",
    "                print( 'Training Loss in epoch {} is {}:'.format(i+1,avg_loss/(j+1) ))\n",
    "                training_loss.write('Training Loss for Epoch {} is {}:\\n'.format((i+1) , avg_loss/(j+1)))\n",
    "\n",
    "\n",
    "\n",
    "    #         Validation pipeline\n",
    "                validation = pipeline.train_input( validation_filepath, BATCH_SIZE, 1 )\n",
    "                up_audio_valid, down_audio_valid = validation\n",
    "\n",
    "                train_flag_val, x_val , up_audio_return_y_valid = models.inference( input_data = \n",
    "                                                                                            up_audio_valid , \n",
    "                                                                        input_shape = up_audio_eval.shape)\n",
    "                loss_valid = losses.mse(\"loss_valid\",up_audio_valid, up_audio_return_y_valid)\n",
    "                num_batches_per_epoch_valid = int(SAMPLES_PER_EPOCH_VALID/BATCH_SIZE)\n",
    "\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                for j in range(num_batches_per_epoch_valid):\n",
    "                    avg_loss = 0\n",
    "                    try:\n",
    "                        loss_value = sess.run([loss_valid])\n",
    "                        print('Loss value:',loss_value)\n",
    "                        avg_loss +=  loss_value[0]\n",
    "\n",
    "                    except tf.errors.OutOfRangeError :\n",
    "                        print('Data used')\n",
    "                        pass\n",
    "                print('Validation loss for epoch {} is {}:'.format(i+1, avg_loss/(j+1)))\n",
    "                valid_loss.write('Validation Loss for Epoch {} is {}:\\n'.format((i+1) , avg_loss/(j+1)))\n",
    "\n",
    "            saver.save(sess, './aux/checkpoint/checkpoint{}.ckpt'.format(i+1),global_step=global_step)\n",
    "            training_loss.close()\n",
    "            valid_loss.close()\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "          \n",
    "\n",
    "print('Process finished')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filelist = os.listdir('./data/upsampling/upsampling_data/')\n",
    "filelist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
